{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efd0f119-604c-4be8-ba24-834565b9e5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`cal total purchase amt for each customers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ef5fa1-afc9-4c3c-945a-e6b9f9bcaaad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "#df.show()\n",
    "total_purchase_customer = df.groupBy(\"customer_id\").agg(F.sum(\"purchase_amount\").alias(\"total_purchase_amt\"))\n",
    "total_purchase_customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1191fb14-61de-4858-84af-a2f2f87644a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.FIND THE CUSTOMER WITH THE HIGHEST\n",
    " TOTAL PURHASE AMOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c055e7-1628-4a83-a260-f7cdfbaf88b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "total_purchase_per_customer = df.groupBy(\"customer_id\").agg(F.sum(\"purchase_amount\").alias(\"total_purchase_amount\"))\n",
    "customer_with_highest_purchase = total_purchase_per_customer.orderBy(F.desc(\"total_purchase_amount\")).first()\n",
    "#df.show()\n",
    "#total_purchase_per_customer.show()\n",
    "print(\"Customer with highest purchase:\", customer_with_highest_purchase[\"customer_id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7c13a3f-556c-4a4e-84b8-e156af991567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3.Calculate the total revenue generated from all sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0837aa47-81d6-4c0f-b807-bda7f8cd3217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg,expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Sales\").getOrCreate()\n",
    "\n",
    "schema = [\"product_id\", \"product_name\", \"category\", \"price\", \"quantity_sold\"]\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\", 500, 100),\n",
    "    (2, \"Product B\", \"Clothing\", 50, 200),\n",
    "    (3, \"Product C\", \"Electronics\", 800, 50),\n",
    "    (4, \"Product D\", \"Beauty\", 30, 300),\n",
    "    (5, \"Product E\", \"Clothing\", 75, 150)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "#df.show()\n",
    "\n",
    "# Question 1: Calculate total revenue\n",
    "\n",
    "total_revenue = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity_sold\")).agg(sum(\"revenue\")).collect()[0][0]\n",
    "print(\"Total Revenue:\", total_revenue)\n",
    "\n",
    "\n",
    "# Question 2: Top 5 best-selling products\n",
    "\n",
    "top_products = df.orderBy(col(\"quantity_sold\",).desc()).limit(5)\n",
    "top_products.show()\n",
    "\n",
    "\n",
    "# Question 3: Average price per category\n",
    "\n",
    "avg_category = df.groupBy(\"category\").agg(avg(\"price\").alias(\"avg_price\"))\n",
    "avg_category.show()\n",
    "\n",
    "\n",
    "# Question 4: Category with highest total revenue\n",
    "\n",
    "total_revenue_category = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity_sold\")).groupBy(\"category\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "highest_category = total_revenue_category.orderBy(col(\"total_revenue\").desc()).first()\n",
    "print(\"highest_category:\",highest_category[\"category\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f29e97-2380-43c3-9896-68f82bd9f046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.Analysing employee salaries:\n",
    "Task:You have a dataset containing information about \n",
    "employee salaries in a company. Your task is to \n",
    "use PySpark to analyze the data and answer a \n",
    "few questions using aggregate functions.\n",
    "\n",
    "Dataset: The dataset is in CSV format and \n",
    "contains the following columns:\n",
    " employee_id, employee_name, department, salary.\n",
    "\n",
    "Questions:\n",
    "\n",
    "Calculate the total payroll cost for the company.\n",
    "\n",
    "Find the average salary for each department.\n",
    "\n",
    "Identify the highest-paid employee and their department.\n",
    "\n",
    "Calculate the total number of employees in each department.\n",
    "\n",
    "Sample Dataset:\n",
    "\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b570918-3b85-4cf5-915e-8ed077c0ca3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,col,count\n",
    "\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "\n",
    "df_1 = spark.createDataFrame(data, columns)\n",
    "#df_1.show()\n",
    "\n",
    "#Calculate the total payroll cost for the company.\n",
    "#sum(salary)\n",
    "total_payroll = df_1.agg(sum(\"salary\").alias(\"toatal_payroll\")).collect()[0][0]\n",
    "#print(total_payroll)\n",
    "\n",
    "# Question 2: Average salary per department\n",
    "#group by(avg)\n",
    "\n",
    "avg_salary = df_1.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "#avg_salary.show() \n",
    "\n",
    "\n",
    "# Question 3: Highest-paid employee and their department\n",
    "hpe = df_1.orderBy(col(\"salary\").desc()).limit(1).select(\"employee_name\",\"department\").first()\n",
    "print(hpe)\n",
    "\n",
    "# Question 4: Total number of employees per department\n",
    "\n",
    "number_of_employees = df_1.groupBy(\"department\").agg(count(\"employee_id\").alias(\"total_emp\"))\n",
    "number_of_employees.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practise_notebook_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
